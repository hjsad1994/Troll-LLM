# H∆∞·ªõng D·∫´n Test GLM Failover Feature

## üìã M·ª•c L·ª•c
1. [C·∫•u h√¨nh Environment](#1-c·∫•u-h√¨nh-environment)
2. [Build v√† Ch·∫°y](#2-build-v√†-ch·∫°y)
3. [Test Cases](#3-test-cases)
4. [Ki·ªÉm tra Log](#4-ki·ªÉm-tra-log)
5. [X·ª≠ l√Ω L·ªói](#5-x·ª≠-l√Ω-l·ªói)

---

## 1. C·∫•u h√¨nh Environment

### Th√™m v√†o `.env` file:

```bash
# GLM Provider (b·∫Øt bu·ªôc ƒë·ªÉ failover ho·∫°t ƒë·ªông)
GLM_API_KEY=c766e3323f504b5da5eaa9b2b971962d.g9e5mUzILgPPvTc7
GLM_ENDPOINT=https://open.bigmodel.cn/api/paas/v4/chat/completions

# B·∫≠t Cache Fallback Detection
CACHE_FALLBACK_DETECTION=true

# B·∫≠t Failover Manager
CACHE_FAILOVER_ENABLED=true

# Ng∆∞·ª°ng k√≠ch ho·∫°t failover (m·∫∑c ƒë·ªãnh: $1.50)
CACHE_FAILOVER_LOSS_THRESHOLD=1.50

# Th·ªùi gian cooldown (m·∫∑c ƒë·ªãnh: 15 ph√∫t)
CACHE_FAILOVER_COOLDOWN_MINUTES=15

# Cache detection config (ƒë·ªÉ test nhanh)
CACHE_FALLBACK_THRESHOLD_COUNT=1
CACHE_FALLBACK_TIME_WINDOW_MIN=1
```

---

## 2. Build v√† Ch·∫°y

```bash
# V√†o th∆∞ m·ª•c goproxy
cd goproxy

# Build
go build -o goproxy.exe

# Ch·∫°y server
./goproxy.exe
```

### Ki·ªÉm tra Startup Log:

```
‚úÖ GLM provider configured for failover
‚úÖ Failover Manager Enabled: threshold=$1.50, cooldown=15 minutes
‚úÖ Detection enabled: cache_threshold=1, error_threshold=6, window=1m
```

N·∫øu th·∫•y:
- `‚ö†Ô∏è GLM configuration failed` ‚Üí Ki·ªÉm tra `GLM_API_KEY`
- `üîï Failover Manager Disabled` ‚Üí Ki·ªÉm tra `CACHE_FAILOVER_ENABLED=true`

---

## 3. Test Cases

### Test 1: Request Th∆∞·ªùng (Normal Flow)

**M·ª•c ti√™u:** Verify request ƒë∆∞·ª£c route ƒë·∫øn OhMyGPT b√¨nh th∆∞·ªùng

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5-20250929",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10
  }'
```

**Log mong ƒë·ª£i:**
```
üîÄ [Model Routing] claude-sonnet-4-5-20250929 -> OhMyGPT (upstream=ohmygpt)
```

**Response:** Model name gi·ªØ nguy√™n `claude-sonnet-4-5-20250929`

---

### Test 2: Anthropic Format

```bash
curl -X POST http://localhost:8080/v1/messages \
  -H "anthropic-version: 2023-06-01" \
  -H "Content-Type: application/json" \
  -H "x-api-key: YOUR_API_KEY" \
  -d '{
    "model": "claude-sonnet-4-5-20250929",
    "max_tokens": 10,
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

---

### Test 3: Trigger Failover Th·ªß C√¥ng

ƒê·ªÉ trigger failover, c·∫ßn m·ªôt request v·ªõi:
- Cache miss: `cache_read = 0` V√Ä `cache_creation = 0`
- Input tokens l·ªõn (>6000)
- Estimated loss > $1.50

**Python Script Test:**

```python
import requests

# Large request (~10K tokens)
large_text = "Explain this: " + "word " * 8000

response = requests.post(
    "http://localhost:8080/v1/chat/completions",
    headers={
        "Authorization": "Bearer YOUR_API_KEY",
        "Content-Type": "application/json"
    },
    json={
        "model": "claude-sonnet-4-5-20250929",
        "messages": [{"role": "user", "content": large_text}],
        "max_tokens": 100
    }
)

print(response.json())
```

**Log khi failover k√≠ch ho·∫°t:**
```
‚ö†Ô∏è [Cache Fallback] Event recorded: model=claude-sonnet-4-5-20250929 tokens=8200 loss=$1.8500
‚ö†Ô∏è [Cache Fallback] Trigger conditions met for claude-sonnet-4-5-20250929: cache_miss=true, loss=$1.85 (threshold=$1.50)
üîÑ [Cache Fallback] Failover activated for claude-sonnet-4-5-20250929 due to cache loss threshold exceeded
üîÑ [Failover Manager] Activated for claude-sonnet-4-5-20250929 (until 2025-01-09 15:30:00, trigger #1)
```

---

### Test 4: Verify Routing sau Failover

Sau khi failover ƒë∆∞·ª£c k√≠ch ho·∫°t, c√°c request ti·∫øp theo s·∫Ω route ƒë·∫øn GLM:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5-20250929",
    "messages": [{"role": "user", "content": "Test"}],
    "max_tokens": 10
  }'
```

**Log mong ƒë·ª£i:**
```
üîÑ [Model Routing] claude-sonnet-4-5-20250929 -> GLM (failover active until 15:30:05)
üì§ [GLM-OpenAI] Forwarding /v1/chat/completions (model=claude-sonnet-4-5-20250929, stream=false, failover=true)
```

**Response:** Model name V·∫™N l√† `claude-sonnet-4-5-20250929` (ƒë√£ ƒë∆∞·ª£c rewrite t·ª´ GLM response)

---

### Test 5: Auto-Recovery sau Cooldown

Sau 15 ph√∫t (ho·∫∑c ch·ªânh `CACHE_FAILOVER_COOLDOWN_MINUTES=1` ƒë·ªÉ test nhanh), request ƒë·∫ßu ti√™n sau cooldown s·∫Ω:

1. Th·ª≠ route l·∫°i OhMyGPT
2. N·∫øu cache OK ‚Üístay v·ªõi OhMyGPT
3. N·∫øu cache v·∫´n fail ‚Üí trigger l·∫°i failover

**Log khi auto-recovery:**
```
‚úÖ [Failover Manager] Auto-recovered for claude-sonnet-4-5-20250929 (cooldown expired)
üîÄ [Model Routing] claude-sonnet-4-5-20250929 -> OhMyGPT (upstream=ohmygpt)
```

---

## 4. Ki·ªÉm tra Log

### Success Indicators:

| Indicator | Log | √ù nghƒ©a |
|-----------|-----|---------|
| ‚úÖ GLM configured | `GLM provider configured for failover` | GLM ƒë√£ ƒë∆∞·ª£c init |
| ‚úÖ Failover enabled | `Failover Manager Enabled: threshold=$1.50` | Failover manager ƒëang ch·∫°y |
| ‚úÖ Normal routing | `-> OhMyGPT (upstream=ohmygpt)` | Route b√¨nh th∆∞·ªùng |
| ‚úÖ Failover active | `-> GLM (failover active until HH:MM:SS)` | ƒêang failover |
| ‚úÖ Auto-recovered | `Auto-recovered for <model>` | ƒê√£ recovery |

### Warning/Error Indicators:

| Indicator | Log | Action |
|-----------|-----|--------|
| ‚ö†Ô∏è GLM not configured | `GLM configuration failed` | Check `GLM_API_KEY` |
| ‚ö†Ô∏è Failover disabled | `Failover Manager Disabled` | Set `CACHE_FAILOVER_ENABLED=true` |
| ‚ö†Ô∏è Fallback to OhMyGPT | `failover active but GLM not configured` | Configure GLM |

---

## 5. X·ª≠ l√Ω L·ªói

### Error: `GLM not configured`

```
‚ùå [GLM-OpenAI] GLM not configured
```

**Fix:** Th√™m v√†o `.env`
```bash
GLM_API_KEY=c766e3323f504b5da5eaa9b2b971962d.g9e5mUzILgPPvTc7
```

---

### Error: Failover kh√¥ng k√≠ch ho·∫°t

**Ki·ªÉm tra:**
1. `CACHE_FALLBACK_DETECTION=true` ?
2. `CACHE_FAILOVER_ENABLED=true` ?
3. Cache miss detected? Check log:
   ```
   ‚ö†Ô∏è [Cache Fallback] Event recorded: model=... tokens=... loss=$...
   ```

---

### Error: Request timeout khi failover

**Nguy√™n nh√¢n:** GLM API endpoint kh√¥ng ·ªïn ƒë·ªãnh

**Fix:** Th·ª≠ endpoint kh√°c:
```bash
GLM_ENDPOINT=https://api.z.ai/api/paas/v4/chat/completions
```

---

## 6. Quick Test Commands

```bash
# Test nhanh (copy-paste)
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer YOUR_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"claude-sonnet-4-5-20250929","messages":[{"role":"user","content":"Hi"}],"max_tokens":10}'
```

```python
# Python quick test
import requests
r = requests.post(
    "http://localhost:8080/v1/chat/completions",
    headers={"Authorization": "Bearer YOUR_KEY", "Content-Type": "application/json"},
    json={"model": "claude-sonnet-4-5-20250929", "messages": [{"role": "user", "content": "Hi"}], "max_tokens": 10}
)
print(r.json())
```

---

## 7. Checklist Tr∆∞·ªõc Production

- [ ] GLM API key ƒë√£ c√≥ v√† ho·∫°t ƒë·ªông
- [ ] Test v·ªõi request nh·ªè ‚Üí OK
- [ ] Test v·ªõi request l·ªõn ‚Üí Trigger failover OK
- [ ] Verify model name preservation OK
- [ ] Test streaming OK
- [ ] Test Anthropic format OK
- [ ] Monitor logs cho 24h v·ªõi staging traffic
- [ ] Adjust threshold n·∫øu c·∫ßn ($1.50 ‚Üí $5.00 ƒë·ªÉ conservative)

---

## 8. Monitoring trong Production

### Metrics c·∫ßn theo d√µi:

1. **Failover Activation Count:**
   ```
   grep "Failover activated" goproxy.log | wc -l
   ```

2. **Auto-Recovery Count:**
   ```
   grep "Auto-recovered" goproxy.log | wc -l
   ```

3. **GLM Request Success Rate:**
   ```
   grep "GLM-OpenAI" goproxy.log | grep -c "200"
   ```

4. **Cache Fallback Events:**
   ```
   grep "Cache Fallback.*Event recorded" goproxy.log
   ```

---

**Last Updated:** 2025-01-09
**Contact:** Development Team
