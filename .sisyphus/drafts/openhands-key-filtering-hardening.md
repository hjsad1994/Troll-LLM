# Draft: OpenHands Upstream Key Selection Hardening

## User Request
"Ensure when user requests to OpenHands upstream, they don't use keys with need_refresh or exceeded/exhausted status"

Vietnamese: "ultrawork dam bao user khi request len openhands upstream thi khong request vao key bi need_refresh hoac exccuted"

## Current Architecture Analysis

### ‚úÖ Existing Safeguards (VERIFIED)

1. **Filter Logic (`openhandspool/model.go:26-39`)**
   ```go
   func (k *OpenHandsKey) IsAvailable() bool {
       // Line 28-30: BLOCKS exhausted and need_refresh
       if k.Status == StatusExhausted || k.Status == StatusNeedRefresh {
           return false
       }
       // Cooldown logic only applies to rate_limited/error status
       if k.Status != StatusHealthy {
           if k.CooldownUntil != nil && time.Now().After(*k.CooldownUntil) {
               return true // Cooldown expired
           }
           return false
       }
       return true
   }
   ```

2. **Key Selection (`openhandspool/pool.go:126-147`)**
   - `SelectKey()` always calls `key.IsAvailable()` before returning
   - Round-robin only considers available keys
   - Returns `ErrNoHealthyKeys` if all keys filtered out

3. **Handler Integration (`main.go:1220, 1423`)**
   - `handleOpenHandsMessagesRequest()` uses `openhandsPool.SelectKey()`
   - Retry logic also uses `SelectKey()` (no bypass)

### ‚ö†Ô∏è Identified Vulnerabilities

#### **Race Condition #1: Async DB Update** (CRITICAL)
**Location**: `openhandspool/pool.go:149-169`

```go
func (p *KeyPool) MarkStatus(...) {
    p.mu.Lock()
    key.Status = status  // In-memory update
    p.mu.Unlock()
    
    go p.updateKeyStatus(...)  // ‚ùå ASYNC DB write
}
```

**Impact**: Status update visible to SelectKey() immediately, but DB write delayed. If auto-reload happens before DB write completes, status reverts to stale DB value.

**Reproduction Scenario**:
1. T=0ms: Request arrives, SelectKey() picks Key A (healthy)
2. T=10ms: Key A fails 401, MarkStatus() sets status=need_refresh IN MEMORY
3. T=12ms: Auto-reload fires (runs every 60s), calls LoadKeys()
4. T=13ms: LoadKeys() reads from DB (still shows "healthy" - DB write at T=10ms hasn't completed)
5. T=14ms: In-memory status **REVERTS** to healthy
6. T=20ms: Next request picks Key A again ‚ùå

#### **Race Condition #2: Reload Overwrites In-Memory State** (HIGH)
**Location**: `openhandspool/pool.go:295-309`

Auto-reload unconditionally replaces in-memory state with DB state every N seconds. No merging logic.

#### **Race Condition #3: Rotation Synchronization** (MEDIUM)
**Location**: `openhandspool/rotation.go:116-132`

After rotation completes (DB updated), in-memory pool update happens under mutex. No synchronization barrier for other goroutines that already grabbed old key reference.

### üéØ Root Cause Hypothesis

**Most likely**: Auto-reload timing window causes status reversion.

**Evidence**:
- User reported this in production (not just theory)
- Auto-reload runs every 60 seconds (hardcoded)
- Async DB writes typically take 5-50ms
- Window of vulnerability: ~50-100ms per status change

## Decisions Made

### ‚úÖ Confirmed Requirements
1. Keys with `status="need_refresh"` MUST NEVER be selected
2. Keys with `status="exhausted"` MUST NEVER be selected
3. Guarantee must hold under high concurrency (multiple requests/sec)
4. Guarantee must hold even during auto-reload cycles

### ‚úÖ Technical Approach Decided
1. **Make MarkStatus synchronous** - Wait for DB write before returning
2. **Add defensive pre-flight check** - Re-validate key before each request
3. **Add monitoring** - Log warnings if filtering fails
4. **Add integration test** - Verify under concurrent load

### ‚ö†Ô∏è Open Questions
None - all requirements clear.

## Scope Boundaries

### IN SCOPE
- Fixing race conditions in openhandspool package
- Adding defensive checks in main.go request handlers
- Adding integration tests for concurrent scenarios
- Documentation of guarantees

### OUT OF SCOPE
- Fixing similar issues in openhands package (legacy, not actively used)
- Changing auto-reload interval (60s is acceptable)
- Refactoring key pool architecture (too large, risky)
- Adding distributed locking (MongoDB not suited for this)

## Test Strategy

**Framework**: Go native testing (`go test`)

**Test Types**:
1. **Unit tests**: Test IsAvailable() logic with all status combinations
2. **Race tests**: Run with `-race` flag to detect data races
3. **Integration tests**: Simulate concurrent requests + auto-reload
4. **Manual verification**: Deploy to staging, mark key exhausted, verify no usage

## Verification Results

### ‚úÖ Verify-1: Code Path Tracing Complete

**All key selection paths traced. Result: IsAvailable() is ALWAYS called.**

#### OpenHandsPool (Primary - Active Use)
**Location**: `goproxy/internal/openhandspool/pool.go:126-147`

```go
func (p *KeyPool) SelectKey() (*OpenHandsKey, error) {
    p.mu.Lock()
    defer p.mu.Unlock()
    
    // Round-robin through available keys
    for i := 0; i < len(p.keys); i++ {
        idx := (startIdx + i) % len(p.keys)
        key := p.keys[idx]
        
        if key.IsAvailable() {  // ‚úÖ ALWAYS called
            p.current = (idx + 1) % len(p.keys)
            return key, nil
        }
    }
    
    return nil, ErrNoHealthyKeys
}
```

**Call Sites in main.go**:
1. Line 1220: `handleOpenHandsMessagesRequest()` - Direct call
2. Line 1423: Retry after rotation (non-streaming only)
3. Line 1542: `handleOpenHandsCompletionsRequest()` - Direct call  
4. Line 1740: Retry after rotation (non-streaming only)

**Verdict**: ‚úÖ All paths call IsAvailable() in SelectKey() loop

---

#### OpenHandsProvider (Legacy - Proxy-Based)
**Location**: `goproxy/internal/openhands/openhands.go:209-230`

```go
func (p *OpenHandsProvider) SelectKey() (*OpenHandsKey, error) {
    p.mu.Lock()
    defer p.mu.Unlock()
    
    for i := 0; i < len(p.keys); i++ {
        idx := (startIdx + i) % len(p.keys)
        key := p.keys[idx]
        
        if key.IsAvailable() {  // ‚úÖ ALWAYS called
            p.current = (idx + 1) % len(p.keys)
            p.lastUsedKeyID = key.ID
            return key, nil
        }
    }
    
    return nil, fmt.Errorf("no healthy OpenHands keys available")
}
```

**Call Sites**:
1. `selectProxyAndKey()` line 594: Fallback when no proxy
2. `selectProxyAndKey()` line 605: Fallback when proxy select fails
3. `selectProxyAndKey()` line 616: Fallback when transport creation fails
4. `selectProxyAndKey()` line 657: Fallback when no binding found

**Special Case - Binding Selection** (line 647):
```go
key := p.GetKeyByID(binding.OpenHandsKeyID)
if key != nil && key.IsAvailable() {  // ‚úÖ ALWAYS checked
    return client, selectedProxy.Name, key, nil
}
```

**Verdict**: ‚úÖ All paths (including binding-based selection) call IsAvailable()

---

#### Summary: Code Path Analysis

| Pool Type | Implementation | IsAvailable() Called? | Bypass Paths? |
|-----------|----------------|----------------------|---------------|
| openhandspool.KeyPool | Round-robin | ‚úÖ YES (line 140) | ‚ùå NONE |
| openhands.OpenHandsProvider | Round-robin | ‚úÖ YES (line 222) | ‚ùå NONE |
| openhands.OpenHandsProvider | Binding-based | ‚úÖ YES (line 647) | ‚ùå NONE |

**Conclusion**: The filtering logic is CORRECTLY enforced at all selection points. No bypass paths exist.

**The bug is NOT in the selection logic - it's in the status update/reload race condition.**

---

## Research Findings

### From Librarian Agent (Go Concurrency Best Practices)
- Async goroutines without synchronization are common source of bugs
- Best practice: Complete critical state changes synchronously
- Use channels or WaitGroup if async is required
- RWMutex can help but doesn't solve async DB write issues

### From Explore Agent (Codebase Analysis)
- Two separate pools exist: `openhandspool` (active) and `openhands` (legacy)
- Both use same filtering logic but openhandspool is primary
- Auto-reload configured in main.go startup
- No existing tests for concurrent scenarios

## Architecture Notes

### Pool Reload Strategy
- **Current**: Full replacement every 60 seconds
- **Issue**: Loses in-memory state changes not yet persisted to DB
- **Fix**: Make MarkStatus synchronous (ensures DB is source of truth)

### Why Not Use Channels?
- Pool is singleton, accessed from multiple handlers
- Mutex is appropriate for this access pattern
- Channel approach would require major refactoring (out of scope)

---

**Last Updated**: 2026-02-01
**Status**: Draft Complete - Ready for Plan Generation
